import java.util.*;
import java.util.concurrent.*;

class Job {
    private ExecutorService service = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());//setup thread pool
    private final ArrayList<CombinerOutput> groupedByKey = new ArrayList<>();//holds information about the combiner output stored in objects
    private final ArrayList<InputReader> inputReaders = new ArrayList<>();//store input reader instances
    private final ArrayList<Mapper> mappers = new ArrayList<>();//storage for mappers objects
    private final ArrayList<Sorter> sorters = new ArrayList<>();//storage for sorters objects
    private final ArrayList<Reducer> reducers = new ArrayList<>();//storage for reducers objects
    private final ArrayList<Combiner> combiners = new ArrayList<>();//storage for combiner objects
    private final Logger logger = new Logger();
    private final Config config;
    private long startTime;//set with start time of map reduce

    /**
     * Job is initialised with a configuration object which holds all information for proper operation
     */
    Job(Config conf){
        this.config = conf;//setup the configuration obj
    }

    /**
     * Takes the input path from the config and runs the parser, with a chunk size (defaults to 256)
     */
    private void reader(){
        for(String filepath: config.getInputPaths()) { //get input files
            inputReaders.add(new InputReader(filepath, config.getChunkSize()));//add a new input reader for each file path
        }
        logger.log("Running Input Reader...");
        if(config.getMultiThreaded()) {
            setupThreadPool();
            for (InputReader p : inputReaders) {
                service.execute(p::run);//run in parallel
            }
            shutdownThreadPool();
        }else{
            for (InputReader p : inputReaders) {
                p.run(); //run the inputReaders sequentially
            }
        }
    }
    /**
     * Iterates over all input reader instances and assigns every chunk from each to a new mapper,
     * runs each map in order, multithreaded or not
     */
    private void map(){
        for(InputReader inputReader : inputReaders) {
            for (ArrayList<String> chunk : inputReader.returnChunks()) {
                mappers.add(new Mapper(chunk, config.getMapperClass())); //give each chunk to a different mapper with a new context
            }
        }
        logger.log("Mappers: " + mappers.size());
        logger.log("Running Mappers...");
        if(config.getMultiThreaded()) {
            setupThreadPool();
            for(Mapper map: mappers){
                service.execute(map::run);//run mappers parallel
            }
            shutdownThreadPool();
        }else{
            for(Mapper map: mappers){
                map.run();//run sequentially
            }
        }
    }

    /**
     * The keys-value pairs generated by the mapper are sorted, i.e. Before the starting of reducers,
     * all intermediate key-value pairs that are generated by the mapper get sorted by key and not by value.
     */
    private void sort(){
        logger.log("Running Sort...");
        for (Mapper map : mappers) {//create a sorter for each mapper's intermediate output (list of key-value pairs)
            sorters.add(new Sorter(map.getIntermediateOutput()));//add a new sorter with the map's intermediate output
        }
        if(config.getMultiThreaded()) {//run in parallel if requested by the user
            setupThreadPool();
            for(Sorter s: sorters){
                service.execute(s::sort);//run in parallel
            }
            shutdownThreadPool();
        }else{//else run sequentially
            for(Sorter s: sorters){
                s.sort();
            }
        }
    }
    /**
     * Takes the buffer from each Mapper and groups values for the same key.
     * A combiner for every mapper
     */
    private void combine(){
        for (Mapper map : mappers) {//create a new combiner for each mapper
            combiners.add(new Combiner(map.getIntermediateOutput(), groupedByKey));//initialise new combiner obj with the mapper's intermediate output and storage for grouping
        }
        logger.log("Running Combiners...");
        if(config.getMultiThreaded()) {
            setupThreadPool();
            for(Combiner c: combiners){//run combiner's combine() method in parallel
                service.execute(c::combine);
            }
            shutdownThreadPool();
        }else{
            for(Combiner c: combiners){//else run sequentially
                c.combine();
            }
        }
    }

    /**
     * This method simply creates the number of reduce tasks set by the user (setNumReduceTasks(int n))
     */
    private void createReducers(){
        for(int i = 0; i < config.numReduceTasks(); i++){
            reducers.add(new Reducer(config.getReducerClass()));//create the number of reduce tasks needed (set by user, default is 1 task)
        }
    }
    /**
     * Partitioner calculates which reducer a key and list of values should be assigned to.
     * This is a replication of Hadoop's HashPartitioner
     */
    private int partitioner(Object key){
        return (key.hashCode() & Integer.MAX_VALUE) % config.numReduceTasks();
    }
    /**
     * Shuffling ensures that all of the values associated with a specific key go to the same reducer
     * To do this, each (key, list(values)) pair produced from the combiner phase is given to an reducer based on the partitioner's returned number
     * which uses a hash calculation on the key.
     * Each reducer is given (key, list(values)) pairs from the grouped values ArrayList, this ArrayList is populated in the Combiner phase,
     * where values are grouped for the same key.
     */
    private void shuffle(){
        logger.log("Shuffling...");
        if(config.numReduceTasks() > 0) {//check to prevent / 0 errors in partitioner
            for (CombinerOutput combinerOutput : groupedByKey) {//iterate over all saved (key, list(values)) pairs
                reducers.get(partitioner(combinerOutput.getKey()))
                        .addKeyListValues(combinerOutput.getKeyAndValuesPair()); //get the reducer number from partitioner and add grouped-by-key (key, (list)value) pair to that reducer
            }
        }
    }
    /**
     * Runs the reducers
     */
    private void reduce(){
        logger.log("Reducing...");
        if(config.getMultiThreaded()) {//if multithreading enabled
            setupThreadPool();
            for(Reducer r : reducers){//run reduce methods on thread pool
                service.execute(r::reduce);
            }
            shutdownThreadPool();
        }else{//else run contiguously
            for(Reducer r : reducers) {
                r.reduce();
            }
        }
    }
    /**
     * This is the output, writes every key/value pair to file, this is the final step
     */
    @SuppressWarnings("unchecked")
    private void output(){
        OutputWriter out = new OutputWriter();//create new object
        out.setFilepath(config.getOutputPath());//setup output path
        for(Reducer r: reducers){//iterate over all reducers
            out.addContext(r.getFinalKeyPairs());//add the context from the reducer to the writer
        }
        out.write();//call write method
    }
    /**
     * This is the driver, performs all actions in order
     */
    void runJob(){
        setStartTime(System.currentTimeMillis());
        reader(); //parse input data into chunks
        map(); //run all mappers in mapper array
        sort();//sort intermediate output
        combine();//combine all key, value pairs to (key, (list)values) based on key
        createReducers();
        shuffle();//assign mappers IO to reducers
        reduce();//reduce
        output();//write to file
        logger.log("Completed Job: " + "'" + config.getJobName() + "'" + " to "
                + config.getOutputPath() + " in " + getElapsed()
                + "ms");//print execution time message
    }

    /**
     * setup the thread pool if the executor service (thread pool) is shut down
     */
    private void setupThreadPool(){
        if(service.isShutdown()){
            service = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());//create new
        }
    }

    /**
     * shutdown the thread pool, this ensures that, after this method has completed, all tasks have completed
     * and the process can continue (this method waits for the thread pool to complete all tasks assigned to it)
     */
    private void shutdownThreadPool() {
        service.shutdown();
        try {
            service.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);
        } catch (InterruptedException e) {
            logger.log(e.getMessage());
        }
    }
    private void setStartTime(long l){
        startTime = l;
    }

    /**
     * Calculate execution time
     */
    private long getElapsed(){
        return System.currentTimeMillis() - startTime;
    }
}