import java.util.*;
import java.util.concurrent.*;

/**
 * Created by Oliver on 18/11/2017.
 * Written by Oliver Bathurst <oliverbathurst12345@gmail.com>
 */

class Job {
    private ExecutorService service = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());
    private final ArrayList<CombinerOutput> groupedValues = new ArrayList<>();
    private final ArrayList<InputReader> inputReaders = new ArrayList<>();
    private final ArrayList<Mapper> mappers = new ArrayList<>();
    private final ArrayList<Reducer> reducers = new ArrayList<>();
    private final ArrayList<Combiner> combiners = new ArrayList<>();
    private final Logger logger = new Logger();
    private final Config config;
    private long startTime;

    Job(Config conf){
        this.config = conf;
    }

    /**
     * Takes the input path from the config and runs the parser, with a chunk size (defaults to 128)
     */
    private void reader(){
        for(String filepath: config.getInputPaths()) { //get input files
            inputReaders.add(new InputReader(filepath, config.getChunkSize()));
        }
        logger.log("Running Input Reader...");
        if(config.getMultiThreaded()) {
            setupThreadPool();
            for (InputReader p : inputReaders) {
                service.execute(p::run);
            }
            shutdownThreadPool();
        }else{
            for (InputReader p : inputReaders) {
                p.run(); //run the inputReaders
            }
        }
    }
    /**
     * runs each map in order
     */
    private void map(){
        for(InputReader inputReader : inputReaders) {
            for (ArrayList<String> chunk : inputReader.returnChunks()) {
                mappers.add(new Mapper(chunk, config.getMapperClass())); //give each chunk to a different mapper with a new context
            }
        }
        logger.log("Mappers: " + mappers.size());
        logger.log("Running Mappers...");
        if(config.getMultiThreaded()) {
            setupThreadPool();
            for(Mapper map: mappers){
                service.execute(map::run);
            }
            shutdownThreadPool();
        }else{
            for(Mapper map: mappers){
                map.run();
            }
        }
    }

    /**
     * The keys-value pairs generated by the mapper are sorted, i.e. Before starting of reducer,
     * all intermediate key-value pairs that are generated by the mapper get sorted by key and not by value.
     */
    private void sort(){
        logger.log("Running Sort...");
        for (Mapper map : mappers) {
            map.getIntermediateOutput().sort((o1, o2) -> o1.getKey().equals(o2.getKey()) ? 0 : 1);
        }
    }
    /**
     * Takes the buffer from each Mapper and groups values for the same key.
     * A combiner for every mapper
     */
    private void combine(){
        for (Mapper map : mappers) {
            combiners.add(new Combiner(map.getIntermediateOutput(), groupedValues));
        }
        logger.log("Running Combiners...");
        if(config.getMultiThreaded()) {
            setupThreadPool();
            for(Combiner c: combiners){
                service.execute(c::combine);
            }
            shutdownThreadPool();
        }else{
            for(Combiner c: combiners){
                c.combine();
            }
        }
    }
    /**
     * Shuffling ensures that all of the values associated with a specific key go to the same reducer
     * To do this, each (key, list(values)) pair produced from the combiner phase is given a unique reducer.
     * Each reducer is initialised with a unique (key, list(values)) pair
     * from the grouped values ArrayList, this ArrayList is populated in the Combiner phase,
     * where values are grouped for the same key.
     *  "Each unique key output from the mappers should be assigned to a unique reducer."
     */
    private void shuffle(){
        logger.log("Shuffling...");
        for (CombinerOutput combinerOutput : groupedValues) {
            reducers.add(new Reducer(combinerOutput.getKeyAndValuesPair(), config.getReducerClass())); //give grouped-by-key intermediate key-value pairs to reducer
        }
    }
    /**
     * runs each reducer in order (multithreading not possible due to reflection)
     */
    private void reduce(){
        logger.log("Reducing...");
        for(Reducer r: reducers){
            r.reduce();
        }
    }
    /**
     * This is the output, writes every key/value pair to file, this is the final step
     */
    @SuppressWarnings("unchecked")
    private void output(){
        OutputWriter out = new OutputWriter();
        out.setFilepath(config.getOutputPath());
        for(Reducer r: reducers){
            out.addContext(r.getFinalKeyPairs());
        }
        logger.log("Writing to disk...");
        out.write();
    }
    /**
     * This is the driver, performs all actions in order
     */
    void runJob(){
        setStartTime(System.currentTimeMillis());
        reader(); //parse input data into chunks
        map(); //run all mappers in mapper array
        sort();
        combine();
        shuffle();//assign mappers IO to reducers
        reduce();
        output();
        logger.log("Completed Job: " + "'" + config.getJobName() + "'" + " to "
                + config.getOutputPath() + " in " + getElapsed()
                + "ms");
    }
    private void setupThreadPool(){
        if(service.isShutdown()){
            service = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());
        }
    }
    private void shutdownThreadPool() {
        service.shutdown();
        try {
            service.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);
        } catch (InterruptedException e) {
            logger.log(e.getMessage());
        }
    }
    private void setStartTime(long l){
        startTime = l;
    }
    private long getElapsed(){
        return System.currentTimeMillis() - startTime;
    }
}